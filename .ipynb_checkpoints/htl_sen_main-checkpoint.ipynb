{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取文件,code编码方式\n",
    "def read_coding(filename, code='utf-8'):\n",
    "    with open(filename, 'r') as fp:\n",
    "        word_list = fp.readlines()\n",
    "        word_encode = [word.strip().decode(code, 'ignore') for word in word_list]\n",
    "    return word_encode\n",
    "\n",
    "# 打印列表\n",
    "def print_list(lst):\n",
    "    print ' '.join(lst)\n",
    "\n",
    "# 全角转半角\n",
    "def full2half(ustring):\n",
    "    rstring = ''\n",
    "    for uchar in ustring:\n",
    "        inside_code = ord(uchar)\n",
    "        if inside_code == 12288:  # 空格      \n",
    "            inside_code = 32\n",
    "        elif inside_code == 12290:  # 句号\n",
    "            inside_code = 46\n",
    "        elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "            inside_code -= 65248\n",
    "        rstring += unichr(inside_code)\n",
    "    return rstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词典预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建基础情感词典\n",
    "- Hownet知网情感词典（积极：4566  消极：4370）：`dict/hownet/*`\n",
    "- NTUSD情感词典（积极：2810  消极：8276）: `dict/`\n",
    "- 中文褒贬义情感词典（积极：5567  消极：4469）: `dict/`\n",
    "\n",
    "将三个词典进行合并去重,并人工筛选出明显错误，最终得到情感词典`dict/fin_neg`和`dict/fin_pos`\n",
    "\n",
    "\n",
    "## 停用词处理\n",
    "- 原始停用词表`dict/stop_words_ch.txt`中去除情感词，否定词，程度副词, 总结词， 关联词， 转折词\n",
    "\n",
    "## 词典结构\n",
    "- dict/fin_neg 负情感词\n",
    "- dict/fin_pos 正情感词\n",
    "- dict/inversedict.txt 否定词\n",
    "- dict/degreeDict.txt 程度副词\n",
    "- dict/stop_words_ch.txt 停用词\n",
    "\n",
    "## 数据集\n",
    "- 谭松波酒店评论语料（10000条），选出其中2000条作为测试集\n",
    "- 李军酒店评论语料（20000条）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并的情感词典\n",
    "neg_list = read_coding('dict/fin_neg')  # 13533\n",
    "pos_list = read_coding('dict/fin_pos')  # 10008\n",
    "\n",
    "# 获取种子词典\n",
    "seeds = read_coding('dict/seed_501')\n",
    "seed_n = []\n",
    "seed_p = []\n",
    "for seed in seeds: \n",
    "    s = seed.split()\n",
    "    if int(s[1]) == 1:\n",
    "        seed_p.append(s[0])\n",
    "    elif int(s[1]) == -1:\n",
    "        seed_n.append(s[0])\n",
    "    else:\n",
    "        print s[0]\n",
    "\n",
    "new_neg_list = neg_list + seed_n  # 13852\n",
    "new_pos_list = pos_list + seed_p  # 10189\n",
    "\n",
    "# 初始情感词典\n",
    "sen_dict = {}\n",
    "for w in neg_list:\n",
    "    sen_dict[w] = -1\n",
    "for w in pos_list:\n",
    "    sen_dict[w] = 1\n",
    "\n",
    "# 关键词\n",
    "guanjian_neg = u'再也不来 下次不来 后悔入住 再也不住 再也不来住 决不住 不会入住 考虑换酒店 下次不会去 别去了'.split()\n",
    "guanjian_pos = u'建议入住 推荐入住 强烈推荐 极力推荐 很值得'.split()\n",
    "for w in guanjian_neg:\n",
    "    sen_dict[w] = -10\n",
    "for w in guanjian_pos:\n",
    "    sen_dict[w] = 10\n",
    "\n",
    "sen_list = sen_dict.keys()\n",
    "\n",
    "# 否定词\n",
    "not_list = read_coding('dict/inversedict.txt')\n",
    "# 程度副词\n",
    "degree_list = read_coding('dict/degreeDict.txt', 'gbk')\n",
    "degree_dict = {}\n",
    "for line in degree_list:\n",
    "    lines = line.split(' ')\n",
    "    degree_dict[lines[0]] = lines[1]\n",
    "degree_list = degree_dict.keys()\n",
    "\n",
    "zongjie = u'总体说来 总体来说 总体看 总体感觉 总的来看 总的说来 总之 总而言之 总结 整体感觉 整体来说 整体说来 整体看 一句话'.split()\n",
    "zhuanzhe = u'但是 却 然而 可是 只是 不过 不料 竟然 偏偏 可惜 岂知 没想到'.split()\n",
    "guanjian = guanjian_neg + guanjian_pos\n",
    "# 停用词\n",
    "stop_words = read_coding('dict/stop_words_ch.txt', 'gbk')\n",
    "\n",
    "# 将总结词，关键词，转折词放入结巴分词词典中，避免被错分\n",
    "words = zongjie + zhuanzhe + guanjian\n",
    "for w in zongjie + zhuanzhe + guanjian:\n",
    "    jieba.add_word(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分句：转化为半角符号，根据逗号，句号，分号，回车，问号分句\n",
    "def text2sen(text):\n",
    "    text = full2half(text)\n",
    "    if u'\\u5bbe\\u9986\\u53cd\\u9988' in text:\n",
    "        text = text.split(u'\\u5bbe\\u9986\\u53cd\\u9988')[0]  # 去除宾馆反馈的影响\n",
    "    sents = re.split(',|\\.|;|\\n|\\?| |', text)\n",
    "    return_text = []\n",
    "    for sent in sents:\n",
    "        if '!' in sent:\n",
    "            return_text.append(sent.split('!')[0]*3)  # 感叹句翻4倍计算\n",
    "        return_text.append(sent)\n",
    "    return return_text\n",
    "\n",
    "# 分词、去除停用词\n",
    "def sent2word(sentence, new_stop_words):\n",
    "    segList = jieba.lcut(sentence)\n",
    "    newSent = [w for w in segList if w not in new_stop_words]\n",
    "    return newSent\n",
    "\n",
    "# 获取语料集，先分句后分词，[[senlist], [senlist]]\n",
    "def corpus2list(path, new_stop_words):\n",
    "    corpus = []\n",
    "    for parent, _, fileNames in os.walk(path):\n",
    "        for fileName in tqdm(fileNames):\n",
    "            currentPath = os.path.join(parent, fileName)\n",
    "            with open(currentPath, 'r') as fp:\n",
    "                text = fp.read().decode('gbk', 'ignore').strip()  # unicode\n",
    "                sens = text2sen(text)  # 分句\n",
    "                sen_list = []  # 每一句分词\n",
    "                for sen in sens:\n",
    "                    cut_sent = sent2word(sen, new_stop_words)  # 分词\n",
    "                    sen_list.append(cut_sent)\n",
    "                corpus.append(sen_list)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算情感分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 词性分类\n",
    "def classifyWords(wordList, sen_dict):\n",
    "    sen_list = sen_dict.keys()\n",
    "    sen_word, not_word, degree_word = {}, {}, {}\n",
    "    for i, word in enumerate(wordList):\n",
    "        if word in sen_list and word not in not_list and word not in degree_list:\n",
    "            sen_word[i] = sen_dict[word]  # {'loc':score}\n",
    "        elif word in not_list and word not in degree_list:\n",
    "            not_word[i] = -1  # {'loc':-1}\n",
    "        elif word in degree_list:\n",
    "            degree_word[i] = degree_dict[word]  # {'loc': score}\n",
    "    return sen_word, not_word, degree_word\n",
    "\n",
    "# 计算句子级别情感分,输入已分词的句子列表\n",
    "def sent_score(wordList, sen_dict):\n",
    "    senWord, notWord, degreeWord = classifyWords(wordList, sen_dict)\n",
    "    senloc = 0 # 上个情感词位置\n",
    "    scoreSum = 0\n",
    "    # 存所有情感词的位置的列表\n",
    "    senLoc = senWord.keys()\n",
    "    notLoc = notWord.keys()\n",
    "    degreeLoc = degreeWord.keys()\n",
    "\n",
    "    for i in range(len(wordList)): # 遍历每一个词\n",
    "        # 如果是情感词\n",
    "        score = 0\n",
    "        if i in senLoc:\n",
    "            score += float(senWord[i])\n",
    "            for j in range(senloc , i): # 在两个情感词之间查找否定词与程度副词\n",
    "                if j in notLoc: # 存在否定词\n",
    "                    score *= -1\n",
    "                elif j in degreeLoc: # 存在程度副词\n",
    "                    score *= float(degreeWord[j])\n",
    "                elif j in zhuanzhe:\n",
    "                    score *= 3\n",
    "                elif j in zongjie:\n",
    "                    score *= 2\n",
    "            senloc = i\n",
    "            scoreSum += score\n",
    "    return scoreSum\n",
    "\n",
    "# 篇章级别情感分\n",
    "def doc_score(test, sen_dict):\n",
    "    score_all = []\n",
    "    for doc in tqdm(test):\n",
    "        score = 0\n",
    "        for sent in doc:\n",
    "            score += sent_score(sent, sen_dict)\n",
    "        score_all.append(score)\n",
    "    return score_all\n",
    "\n",
    "# 一个文本的情感得分\n",
    "def a_doc_score(doc):\n",
    "    score = 0\n",
    "    for sent in doc:\n",
    "        score += sent_score(sent)\n",
    "    return score\n",
    "\n",
    "# 输出情感分以便观察\n",
    "def print_score(cor):\n",
    "    for doc in cor:\n",
    "        score = a_doc_score(doc)\n",
    "        if score > 0:\n",
    "            print \n",
    "            print u'总得分：', score\n",
    "            print \n",
    "            for sen in doc:\n",
    "                print sent_score(sen),\n",
    "                print_list(sen)\n",
    "    return 0\n",
    "\n",
    "def get_result(neg_score, pos_score):\n",
    "    np_neg = np.array(neg_score)\n",
    "    np_pos = np.array(pos_score)\n",
    "    np_all = np.append(np_neg, np_pos)\n",
    "    accuracy = np.append((np_neg < 0), (np_pos > 0)).astype('int').mean()\n",
    "    precision_neg = sum(np_neg < 0)*1.0/sum(np_all < 0)\n",
    "    precision_pos = sum(np_pos > 0)*1.0/sum(np_all > 0)\n",
    "    recall_neg = sum(np_neg < 0)*1.0/len(np_neg)\n",
    "    recall_pos = sum(np_pos > 0)*1.0/len(np_pos)\n",
    "    f1_neg = 2*precision_neg*recall_neg/(precision_neg+recall_neg + 1e-8) # f1值\n",
    "    f1_pos = 2*precision_pos*recall_pos/(precision_pos+recall_pos + 1e-8)\n",
    "    print 'accuracy: ', accuracy\n",
    "    print 'precision_neg: ', precision_neg\n",
    "    print 'precision_pos: ', precision_pos\n",
    "    print 'recall_neg: ', recall_neg\n",
    "    print 'recall_pos: ', recall_pos\n",
    "    print 'f1_neg: ', f1_neg\n",
    "    print 'f1_pos: ', f1_pos\n",
    "    return accuracy, precision_neg, precision_pos, recall_neg, recall_pos\n",
    "\n",
    "# print_score(test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sen_main(neg_list, pos_list):\n",
    "    sen_dict = {}\n",
    "    for w in neg_list:\n",
    "        sen_dict[w] = -1\n",
    "    for w in pos_list:\n",
    "        sen_dict[w] = 1\n",
    "    \n",
    "    # 关键词\n",
    "    for w in guanjian_neg:\n",
    "        sen_dict[w] = -10\n",
    "    for w in guanjian_pos:\n",
    "        sen_dict[w] = 10\n",
    "\n",
    "    sen_list = sen_dict.keys()\n",
    "    \n",
    "    not_stop_words = sen_list + degree_list + not_list + zongjie + zhuanzhe + guanjian\n",
    "    new_stop_words = [word for word in stop_words if word not in not_stop_words]\n",
    "    \n",
    "    test_neg = corpus2list('data/htl_corpus/test/neg', new_stop_words)\n",
    "    test_pos = corpus2list('data/htl_corpus/test/pos',new_stop_words)\n",
    "    \n",
    "    neg_score = doc_score(test_neg, sen_dict)\n",
    "    pos_score = doc_score(test_pos, sen_dict)\n",
    "    \n",
    "    return get_result(neg_score, pos_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 203.81it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 283.52it/s]\n",
      "100%|██████████| 1000/1000 [00:55<00:00, 18.05it/s]\n",
      "100%|██████████| 1000/1000 [00:39<00:00, 19.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.724\n",
      "precision_neg:  0.860563380282\n",
      "precision_pos:  0.738095238095\n",
      "recall_neg:  0.611\n",
      "recall_pos:  0.837\n",
      "f1_neg:  0.714619878185\n",
      "f1_pos:  0.784442356782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_init = sen_main(neg_list, pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 170.47it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 277.46it/s]\n",
      "100%|██████████| 1000/1000 [00:57<00:00, 17.37it/s]\n",
      "100%|██████████| 1000/1000 [00:41<00:00, 24.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7745\n",
      "precision_neg:  0.829347826087\n",
      "precision_pos:  0.814507772021\n",
      "recall_neg:  0.763\n",
      "recall_pos:  0.786\n",
      "f1_neg:  0.794791661675\n",
      "f1_pos:  0.799999995002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_seed = sen_main(new_neg_list, new_pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vector建立词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集\n",
    "- 谭松波酒店评论语料（8000条）\n",
    "- 李军酒店评论语料（20000条）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1991.43it/s]\n",
      "100%|██████████| 6000/6000 [00:03<00:00, 1860.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_text(path):\n",
    "    text_list = []\n",
    "    for parent, _, fileNames in os.walk(path):\n",
    "        for fileName in tqdm(fileNames):\n",
    "            currentPath = os.path.join(parent, fileName)\n",
    "            with open(currentPath, 'r') as fp:\n",
    "                text_list.append(fp.read().decode('gbk', 'ignore').strip())\n",
    "    return text_list\n",
    "\n",
    "with open('data/review_sentiment.txt') as fp:\n",
    "    text = fp.read().decode('gbk', 'ignore').split('ljthunlp')[2:]\n",
    "    pro_text = [sen.split(' train')[0] for sen in text]\n",
    "\n",
    "text_neg_chi = get_text('data/htl_corpus/train/neg')\n",
    "text_pos_chi = get_text('data/htl_corpus/train/pos')\n",
    "text_merge = text_neg_chi + text_pos_chi + pro_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成w2v模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class MyText(object):\n",
    "#     def __iter__(self):\n",
    "#         for line in text_merge:\n",
    "#             yield jieba.lcut(line)\n",
    "\n",
    "# sentences = MyText()\n",
    "# w2v_model = Word2Vec(sentences, size=200, min_count=5, workers=10)\n",
    "# w2v_model.save('data/w2v_ours_200')\n",
    "w2v_model = Word2Vec.load('data/w2v_ours_200')\n",
    "# w2v_sougou_ours_200 全部10W语料\n",
    "# w2v_ours_200 谭+李2W语料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卡方检验进行特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 208.31it/s]\n",
      "100%|██████████| 6000/6000 [00:18<00:00, 322.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 分词不分句\n",
    "def cut_sent(sent_list):\n",
    "    return_sents = []\n",
    "    for sent in tqdm(sent_list):\n",
    "        cuts = jieba.lcut(sent)\n",
    "        cut = [w for w in cuts if w not in stop_words]  # 去除停用词\n",
    "        try:\n",
    "            w1 = cut.index(u'\\u5bbe\\u9986')  # 去除宾馆反馈\n",
    "            w2 = cut.index(u'\\u53cd\\u9988')\n",
    "            if w2-w1 == 1:\n",
    "                cut = cut[:w1]\n",
    "        except:\n",
    "            cut = cut\n",
    "        return_sents.append(cut)\n",
    "    return return_sents\n",
    "# 分词后的语料\n",
    "neg_cut = cut_sent(text_neg_chi)\n",
    "pos_cut = cut_sent(text_pos_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算CHI,去除停用词，停用词对CHI值影响不大，可去可不去。使用stop_words\n",
    "def compute_chi(neg_cut, pos_cut):\n",
    "    neg_corpus_dict = corpora.Dictionary(neg_cut)\n",
    "    pos_corpus_dict = corpora.Dictionary(pos_cut)\n",
    "    n = 8000.0\n",
    "    neg_num = 2000.0\n",
    "    pos_num = 6000.0\n",
    "    dfs_neg = neg_corpus_dict.dfs  # 词id在文档集中出现的文档数\n",
    "    dfs_pos = pos_corpus_dict.dfs\n",
    "    neg_chi = {}\n",
    "    pos_chi = {}\n",
    "    merge_chi = {}\n",
    "    for i, word in neg_corpus_dict.items():\n",
    "        a = dfs_neg[i]\n",
    "        try:\n",
    "            b = dfs_pos[pos_corpus_dict.token2id[word]]\n",
    "        except:\n",
    "            b = 0\n",
    "        c = neg_num - a\n",
    "        d = pos_num - b\n",
    "        x = n*(a*d-b*c)*(a*d-b*c)/neg_num/pos_num/(a+b)/(c+d)  # 计算卡方检验\n",
    "        neg_chi[word] = x\n",
    "\n",
    "    for i, word in pos_corpus_dict.items():\n",
    "        a = dfs_pos[i]\n",
    "        try:\n",
    "            b = dfs_neg[neg_corpus_dict.token2id[word]]\n",
    "        except:\n",
    "            b = 0\n",
    "        c = pos_num - a\n",
    "        d = neg_num - b\n",
    "        x = n*(a*d-b*c)*(a*d-b*c)/neg_num/pos_num/(a+b)/(c+d)  # 计算卡方检验\n",
    "        pos_chi[word] = x\n",
    "    \n",
    "    # 经测试，CHI值相同\n",
    "    for i, word in neg_corpus_dict.items():\n",
    "        neg_chi_score = neg_chi[word]\n",
    "        neg_chi_num = dfs_neg[i]\n",
    "        try:\n",
    "            pos_chi_score = pos_chi[word]\n",
    "            pos_chi_num = dfs_pos[i]\n",
    "        except:\n",
    "            pos_chi_score = 0\n",
    "            pos_chi_num = 0\n",
    "        merge_chi[word] = (neg_chi_score*neg_chi_num + pos_chi_score*pos_chi_num)/n\n",
    "    \n",
    "    for i, word in pos_corpus_dict.items():\n",
    "        pos_chi_score = pos_chi[word]\n",
    "        pos_chi_num = dfs_pos[i]\n",
    "        try:\n",
    "            neg_chi_score = neg_chi[word]\n",
    "            neg_chi_num = dfs_neg[i]\n",
    "        except:\n",
    "            neg_chi_score = 0\n",
    "            neg_chi_num = 0\n",
    "        merge_chi[word] = (neg_chi_score*neg_chi_num + pos_chi_score*pos_chi_num)/n\n",
    "    return merge_chi, neg_chi, pos_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不错 467.129534039\n",
      "不 72.9955600927\n",
      "好 72.2940172515\n",
      "很 61.1338529954\n",
      "说 37.5261589613\n",
      "房间 17.1679198633\n",
      "前台 15.7065076259\n",
      "携程 15.1791901801\n",
      "一个 13.9857686953\n",
      "住 12.754512599\n"
     ]
    }
   ],
   "source": [
    "merge_chi, neg_chi, pos_chi = compute_chi(neg_cut, pos_cut)\n",
    "\n",
    "sort_chi = sorted(merge_chi.items(), key=lambda i:i[1], reverse=True)\n",
    "\n",
    "fin_chi_list = [i[0] for i in sort_chi[:1000]]\n",
    "\n",
    "for i in sort_chi[:10]:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算种子词的情感分\n",
    "- 找出种子词的10个近邻词\n",
    "- 种子词在原始情感词典中，直接取出情感倾向\n",
    "- 种子词不在原始情感词典中，则将种子词的情感分加和，大于零得分1，小于零得分-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 140.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "退 1\n",
      "all -1\n",
      "挂 -1\n",
      "奉劝 -1\n",
      "表扬 1\n",
      "糟糕 -1\n",
      "还要 -1\n",
      "千万 -1\n",
      "整体 1\n",
      "凌晨 -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def seed_score(seed):\n",
    "    w2v_list = w2v_model.wv.vocab.keys()\n",
    "    seed_sen = {}\n",
    "    for w in tqdm(seed):\n",
    "        score  = 0\n",
    "        if w in sen_list:\n",
    "            seed_sen[w] = sen_dict[w]\n",
    "        elif w in w2v_list:\n",
    "            sim_list = w2v_model.wv.most_similar(w, topn=10)\n",
    "            for e in sim_list:\n",
    "                if e[0] in sen_list:\n",
    "                    score += float(sen_dict[e[0]])\n",
    "            seed_sen[w] = 1 if score>0 else -1\n",
    "        else:\n",
    "            seed_sen[w] = 0\n",
    "    return seed_sen\n",
    "\n",
    "seed_sen = seed_score(fin_chi_list)\n",
    "for w,s in seed_sen.items()[:10]:\n",
    "    print w,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算近邻词情感分\n",
    "- 近邻词在原始情感词典中，直接取出情感倾向\n",
    "- 近邻词不在原始情感词典中，则寻找与近邻词最相近的种子词，近邻词的倾向与该种子词相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 19.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "旅舍 0\n",
      "斑驳 -1\n",
      "挂 -1\n",
      "西方 0\n",
      "　0\n",
      "何况 0\n",
      "出来 0\n",
      "第二 1\n",
      "不问 0\n",
      "明快 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_score_by_seed(seed_sen_dict, word):\n",
    "    score = 0\n",
    "    fin_sim = 0\n",
    "    seed_sen_list = seed_sen_dict.keys()\n",
    "    for seed in seed_sen_list:\n",
    "        try:\n",
    "            sim = w2v_model.wv.similarity(word, seed)\n",
    "        except:\n",
    "            sim = 0\n",
    "        if sim > fin_sim:\n",
    "            socre = seed_sen_dict[seed]\n",
    "            fin_sim = sim\n",
    "    return score\n",
    "\n",
    "def neig_score(seed_sen_dict):\n",
    "    neig_sen = {}\n",
    "    seed_sen_list = seed_sen_dict.keys()\n",
    "    w2v_list = w2v_model.wv.vocab.keys()\n",
    "    for w in tqdm(seed_sen_list):\n",
    "        if w not in w2v_list:  # 种子词不在v2w词表中\n",
    "            continue\n",
    "        sim_list = w2v_model.wv.most_similar(w, topn=10)  # 找到10个近邻词\n",
    "        for e in sim_list:\n",
    "            score = 0\n",
    "            if e[0] in neig_sen.keys():  # 近邻词已保存\n",
    "                continue\n",
    "            elif e[0] in sen_list:  # 近邻词在情感词典中\n",
    "                score = sen_dict[e[0]]\n",
    "            elif e[0] in seed_sen_list:  # 近邻词在种子词典中\n",
    "                score = seed_sen_dict[e[0]]\n",
    "            else:  # 通过相似度计算近邻词得分\n",
    "                score = get_score_by_seed(seed_sen_dict, e[0])\n",
    "            neig_sen[e[0]] = score\n",
    "    return neig_sen\n",
    "\n",
    "neig_sen = neig_score(seed_sen)\n",
    "for w,s in neig_sen.items()[:10]:\n",
    "    print w,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_and_neig = seed_sen.copy()\n",
    "seed_and_neig.update(neig_sen)\n",
    "# 输出保存，以便人工筛选\n",
    "with open('dict/seed', 'w') as fp:\n",
    "    for w,s in seed_and_neig.items():\n",
    "        fp.write(w.encode('utf8') + '\\t' + str(s) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
